
@inproceedings{lee_lightweight_2022,
	title = {Lightweight {Neural} {Architecture} {Search} with {Parameter} {Remapping} and {Knowledge} {Distillation}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	url = {https://openreview.net/forum?id=3D2Qz9y001S},
	abstract = {Designing diverse neural architectures taking into account resource constraints or datasets is one of the main challenges in Neural Architecture Search (NAS). However, existing sample-based or one-shot NAS approaches require excessive time or computational cost to be used in multiple practical scenarios. Recently, to alleviate such issues, zero-cost NAS methods that are efficient proxies have been proposed, yet their performance is rather poor due to the strong assumption that they predict the final performance of a given architecture with random initialization. In this work, we propose a novel NAS based on block-wise parameter remapping (PR) and knowledge distillation (KD), which shows high predictive performance while being fast and lightweight enough to be used iteratively to support multiple real-world scenarios. PR significantly shortens training steps and accordingly we can reduce the required time/data for KD to work as an accurate proxy to just few batches, which is largely practical in real-world. In the experiments, we validate the proposed method for its accuracy estimation performance on CIFAR-10 from the MobileNetV3 search space. It outperforms all relevant baselines in terms of performance estimation with only 20 batches.},
	language = {en},
	urldate = {2024-02-27},
	author = {Lee, Hayeon and An, Sohyun and Kim, Minseon and Hwang, Sung Ju},
	month = may,
	year = {2022},
	file = {Full Text PDF:/Users/ansohyeon/Zotero/storage/MWNZMU6I/Lee 등 - 2022 - Lightweight Neural Architecture Search with Parame.pdf:application/pdf},
}

@inproceedings{lee_meta-prediction_2022,
	title = {Meta-prediction {Model} for {Distillation}-{Aware} {NAS} on {Unseen} {Datasets}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	url = {https://openreview.net/forum?id=SEh5SfEQtqB},
	abstract = {Distillation-aware Neural Architecture Search (DaNAS) aims to search for an optimal student architecture that obtains the best performance and/or efficiency when distilling the knowledge from a given teacher model. Previous DaNAS methods have mostly tackled the search for the neural architecture for fixed datasets and the teacher, which are not generalized well on a new task consisting of an unseen dataset and an unseen teacher, thus need to perform a costly search for any new combination of the datasets and the teachers. For standard NAS tasks without KD, meta-learning-based computationally efficient NAS methods have been proposed, which learn the generalized search process over multiple tasks (datasets) and transfer the knowledge obtained over those tasks to a new task. However, since they assume learning from scratch without KD from a teacher, they might not be ideal for DaNAS scenarios. To eliminate the excessive computational cost of DaNAS methods and the sub-optimality of rapid NAS methods, we propose a distillation-aware meta-accuracy prediction model, DaSS (Distillation-aware Student Search), which can predict a given architecture's final performances on a dataset when performing KD with a given teacher, without having actually to train it on the target task. The experimental results demonstrate that our proposed meta-prediction model successfully generalizes to multiple unseen datasets for DaNAS tasks, largely outperforming existing meta-NAS methods and rapid NAS baselines. Code is available at https://github.com/CownowAn/DaSS.},
	language = {en},
	urldate = {2024-02-27},
	author = {Lee, Hayeon and An, Sohyun and Kim, Minseon and Hwang, Sung Ju},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/Users/ansohyeon/Zotero/storage/TEGCRDAE/Lee 등 - 2022 - Meta-prediction Model for Distillation-Aware NAS o.pdf:application/pdf},
}

@misc{an_diffusionnag_2024,
	title = {{DiffusionNAG}: {Predictor}-guided {Neural} {Architecture} {Generation} with {Diffusion} {Models}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	shorttitle = {{DiffusionNAG}},
	url = {http://arxiv.org/abs/2305.16943},
	doi = {10.48550/arXiv.2305.16943},
	abstract = {Existing NAS methods suffer from either an excessive amount of time for repetitive sampling and training of many task-irrelevant architectures. To tackle such limitations of existing NAS methods, we propose a paradigm shift from NAS to a novel conditional Neural Architecture Generation (NAG) framework based on diffusion models, dubbed DiffusionNAG. Specifically, we consider the neural architectures as directed graphs and propose a graph diffusion model for generating them. Moreover, with the guidance of parameterized predictors, DiffusionNAG can flexibly generate task-optimal architectures with the desired properties for diverse tasks, by sampling from a region that is more likely to satisfy the properties. This conditional NAG scheme is significantly more efficient than previous NAS schemes which sample the architectures and filter them using the property predictors. We validate the effectiveness of DiffusionNAG through extensive experiments in two predictor-based NAS scenarios: Transferable NAS and Bayesian Optimization (BO)-based NAS. DiffusionNAG achieves superior performance with speedups of up to 20 times when compared to the baselines on Transferable NAS benchmarks. Furthermore, when integrated into a BO-based algorithm, DiffusionNAG outperforms existing BO-based NAS approaches, particularly in the large MobileNetV3 search space on the ImageNet 1K dataset.},
	urldate = {2024-02-27},
	publisher = {arXiv},
	author = {An, Sohyun and Lee, Hayeon and Jo, Jaehyeong and Lee, Seanie and Hwang, Sung Ju},
	month = jan,
	year = {2024},
	note = {arXiv:2305.16943 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICLR 2024},
	file = {arXiv Fulltext PDF:/Users/ansohyeon/Zotero/storage/KNLPF6MK/An 등 - 2024 - DiffusionNAG Predictor-guided Neural Architecture.pdf:application/pdf;arXiv.org Snapshot:/Users/ansohyeon/Zotero/storage/T8D39P5F/2305.html:text/html},
}

@article{wang_mixture--experts_2023,
	title = {Mixture-of-{Experts} in {Prompt} {Optimization}},
	copyright = {CC0 1.0 Universal Public Domain Dedication},
	url = {https://openreview.net/forum?id=sDmjlpphdB},
	abstract = {Large Language Models (LLMs) exhibit strong generalization power in adapting to novel tasks when prompted with language instructions and in-context demos. Since this ability sensitively depends on the quality of prompts, various methods have been explored to automate the instruction design process. While these methods demonstrated promising results, they also restricted the output space of the search problem to a demo-free instruction. Such simplification significantly limits their performance, as a single demo-free instruction might not be able to cover the entire problem space of the targeted task due to its complexity. To alleviate this issue, we adopt the Mixture-of-Expert paradigm to divide the problem space into homogeneous regions, each governed by a specialized expert. To further improve the coverage of each expert, we expand their prompts to contain both an instruction and several demos. A two-phase process is developed to construct the specialized expert for each region: (1) demo assignment: Inspired by the theoretical connection between in-context learning and kernel regression, we group demos into clusters based on their semantic similarity and assign a cluster to each expert; (2) instruction assignment: A region-based joint search is applied to optimize an instruction complementary to the demo cluster for each expert, yielding a synergistic effect. The resulting method, codenamed Mixture-of-Prompts (MoP), outperforms prior art by up to 43\% on benchmark NLP tasks.},
	language = {en},
	urldate = {2024-02-27},
	author = {Wang, Ruochen and An, Sohyun and Cheng, Minhao and Zhou, Tianyi and Hwang, Sung Ju and Hsieh, Cho-Jui},
	month = oct,
	year = {2023},
	file = {Full Text PDF:/Users/ansohyeon/Zotero/storage/JG8AYHEB/Wang 등 - 2023 - Mixture-of-Experts in Prompt Optimization.pdf:application/pdf},
}
